# -*- coding: utf-8 -*-
"""multi_model_multi_heatmap.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Kppj3URhNuq3ZluQj1g2k6Xhieujsi8
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import the libraries and data"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.14
import numpy as np
import pandas as pd
import os
from glob import glob
# %matplotlib inline
import matplotlib.pyplot as plt
from os import listdir
from sklearn.model_selection import train_test_split
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.mobilenet import MobileNet
from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau
from keras.preprocessing.image import img_to_array
import cv2
from keras.applications import imagenet_utils
import keras.backend as K
from skimage.transform import resize
from keras import Model
from keras.applications import VGG16,DenseNet201,ResNet101,VGG19,InceptionV3

#Read the sample_labels.csv into a pandas dataframe
all_xray_df = pd.read_csv('drive/My Drive/Chest_data/sample/sample_labels.csv')
all_image_paths = {os.path.basename(f): f  for f in listdir("drive/My Drive/Chest_data/sample/images")   }      
print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])
all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)
all_xray_df['Patient Age'] = all_xray_df['Patient Age'].map(lambda x: int(x[:-1]))
all_xray_df.sample(3)

"""## Visualisation"""

label_counts = all_xray_df['Finding Labels'].value_counts()[:15]
fig, ax1 = plt.subplots(1,1,figsize = (12, 8))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 90)

"""## Pre Processing"""

# Since the image may contain multiple disease labels
# Create a list of all disesases and append a new column named output to the x_ray dataframe
all_xray_df['disease_vec']=all_xray_df['Finding Labels'].apply(lambda x: x.split('|'))

# since the dataset is very unbiased, we can resample it to be a more reasonable collection
# weight is 0.1 + number of findings
sample_weights = all_xray_df['Finding Labels'].map(lambda x: len(x.split('|')) if len(x)>0 else 0).values + 4e-2
sample_weights /= sample_weights.sum()
all_xray_df = all_xray_df.sample(5606, weights=sample_weights)

label_counts = all_xray_df['Finding Labels'].value_counts()[:15]
fig, ax1 = plt.subplots(1,1,figsize = (12, 8))
ax1.bar(np.arange(len(label_counts))+0.5, label_counts)
ax1.set_xticks(np.arange(len(label_counts))+0.5)
_ = ax1.set_xticklabels(label_counts.index, rotation = 90)

all_labels=['Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule','Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema','Fibrosis','Pleural Thickening','Hernia']

"""## Splitting the dataset"""

# 20% of the data will be used for testing of model performance
# random state is set so as to get the same split everytime
# stratify is used to have equal proportion of output label in training and validation set
train_df, valid_df = train_test_split(all_xray_df, 
                                   test_size = 0.25, 
                                   random_state = 2018,
                                   stratify = all_xray_df['Finding Labels'].map(lambda x: x[:4]))
print('train', train_df.shape[0], 'validation', valid_df.shape[0])

#creating an Image Data generator
IMG_SIZE = (128, 128)
core_idg = ImageDataGenerator(samplewise_center=True, 
                              samplewise_std_normalization=True, 
                              horizontal_flip = True, 
                              vertical_flip = False, 
                              height_shift_range= 0.05, 
                              width_shift_range=0.1, 
                              rotation_range=5, 
                              shear_range = 0.1,
                              fill_mode = 'reflect',
                              zoom_range=0.15,
                              validation_split = 0.1)

# obtaing the training images using the above generator
train_gen = core_idg.flow_from_dataframe(
        dataframe=train_df,
        directory='drive/My Drive/Chest_data/sample/images',
        x_col='path',
        y_col='disease_vec',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical',subset='training')

# print the output classes in ImageDataGenerator train_gen
train_gen.class_indices.keys()

# obtaing the validation images using the above generator
valid_gen = core_idg.flow_from_dataframe(
        dataframe=train_df,
        directory='drive/My Drive/Chest_data/sample/images',
        x_col='path',
        y_col='disease_vec',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical',subset='validation')

#select a batch of images used for prediction on trained model
test_X, test_Y = next(core_idg.flow_from_dataframe(
        dataframe=train_df,
        directory='drive/My Drive/Chest_data/sample/images',
        x_col='path',
        y_col='disease_vec',
        target_size=(128, 128),
        batch_size=32,
        class_mode='categorical')) # one big batch

t_x, t_y = next(train_gen)
fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))
for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):
    c_ax.imshow(c_x[:,:,0], cmap = 'bone', vmin = -1.5, vmax = 1.5)
    c_ax.set_title(', '.join([n_class for n_class, n_score in zip(all_labels, c_y) 
                             if n_score>0.5]))
    c_ax.axis('off')

"""## Creating model using Keras Functional Model"""

# create a function to return a mobileNet Model with the output layer having 15 output units
def get_mobilenet():
  base_mobilenet_model = MobileNet( input_shape = (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_mobilenet_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_mobilenet_model.inputs,outputs=dense_layer2)
  return model

# create a function to return a denseNet Model with the output layer having 15 output units
def get_densenet():
  base_densenet_model = DenseNet201( input_shape =  (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_densenet_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_densenet_model.inputs,outputs=dense_layer2)
  return model

# create a function to return a inception Model with the output layer having 15 output units
def get_inception():
  base_inception_model = InceptionV3( input_shape =  (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_inception_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_inception_model.inputs,outputs=dense_layer2)
  return model

# create a function to return a resnet Model with the output layer having 15 output units
def get_resnet():
  base_resnet_model = ResNet101( input_shape =  (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_resnet_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_resnet_model.inputs,outputs=dense_layer2)
  return model

# create a function to return a VGG16 Model with the output layer having 15 output units
def get_VGG16():
  base_VGG16_model = VGG16( input_shape =  (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_VGG16_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_VGG16_model.inputs,outputs=dense_layer2)
  return model

# create a function to return a VGG19 Model with the output layer having 15 output units
def get_VGG19():
  base_VGG19_model = VGG19( input_shape =  (128,128,3), include_top = False, weights = None)
  pooling_layer=GlobalAveragePooling2D()(base_VGG19_model.output)
  dropout_layer1=Dropout(0.5)(pooling_layer)
  dense_layer1=Dense(512)(dropout_layer1)
  dropout_layer2=Dropout(0.5)(dense_layer1)
  dense_layer2=Dense(15,activation='sigmoid')(dropout_layer2)
  model=Model(inputs=base_VGG19_model.inputs,outputs=dense_layer2)
  return model

"""##Comparing 6 different Transfer learning Models"""

labels = ["MobileNet","ResNet101","VGG16","InceptionV3","VGG19","DenseNet201"]

models = [get_mobilenet,get_resnet,get_VGG16,get_inception,get_VGG19,get_densenet]

loss = []
binaryAccuracy = []
meanAbsoluteError = []
idx = 0
for model in models:
  # creating a callback to store the best model
  weight_path="{}_weights.best.hdf5".format('xray_'+labels[idx])
  idx += 1

  checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, 
                              save_best_only=True, mode='min', save_weights_only = True)

  early = EarlyStopping(monitor="val_loss", 
                        mode="min", 
                        patience=3)
  callbacks_list = [checkpoint, early]
  multi_disease_model = model()
  multi_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',
                           metrics = ['binary_accuracy', 'mae'])
  multi_disease_model.fit_generator(train_gen, 
                                  steps_per_epoch=5,
                                  validation_data = (test_X, test_Y), 
                                  epochs = 1,
                                  callbacks = callbacks_list)
  
  # load the best weights
  multi_disease_model.load_weights(weight_path)

  # predict on the test_X batch of the validation set
  res = multi_disease_model.evaluate(test_X,test_Y, batch_size = 32, verbose = True)
  loss.append(res[0])
  binaryAccuracy.append(res[1])
  meanAbsoluteError.append(res[2])
  print('loss = ',res[0])
  print('Binary accuracy = ',res[1])
  print('Mean absolute error',res[2])

#clipping loss with very high values
for i in range(0,len(loss)):
  if(loss[i] > 1):
    loss[i] = 1
print(loss)
print(binaryAccuracy)
print(meanAbsoluteError)

"""##Plotting results of different models"""

import matplotlib
import matplotlib.pyplot as plt
import numpy as np


#labels = ["MobileNet","ResNet101","VGG16","InceptionV3","VGG19","DenseNet201"]

x = np.arange(len(labels))  # the label locations
width = 0.25  # the width of the bars

fig, ax = plt.subplots()
rects1 = ax.bar(x - width, loss, width, label='Loss',color='orange')
rects2 = ax.bar(x, binaryAccuracy, width, label='Binary Accuracy',color='green')
rects3 = ax.bar(x +width, meanAbsoluteError, width, label='Mean Absolute Error',color='black')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Value')
ax.set_title('Model')
ax.set_xticks(x)
ax.set_xticklabels(labels)

ax.legend()

"""##Saving the Model Weights"""

# creating a callback to store the best model
weight_path="{}_weights.best.hdf5".format('xray_class')

checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, 
                             save_best_only=True, mode='min', save_weights_only = True)

early = EarlyStopping(monitor="val_loss", 
                      mode="min", 
                      patience=3)
callbacks_list = [checkpoint, early]

"""## Training of the best model"""

# start training the model
model = models[binaryAccuracy.index(max(binaryAccuracy))]
multi_disease_model = model()
multi_disease_model.compile(optimizer = 'adam', loss = 'binary_crossentropy',
                           metrics = ['binary_accuracy', 'mae'])
multi_disease_model.fit_generator(train_gen, 
                                  steps_per_epoch=100,
                                  validation_data = (test_X, test_Y), 
                                  epochs = 2, 
                                  callbacks = callbacks_list)

"""## Testing the performance of the best model"""

# load the best weights
multi_disease_model.load_weights(weight_path)

# predict on the test_X batch of the validation set
res = multi_disease_model.evaluate(test_X,test_Y, batch_size = 32, verbose = True)
print('loss = ',res[0])
print('Binary accuracy = ',res[1])
print('Mean absolute error',res[2])



"""##Making the Prediction"""

pred_Y = multi_disease_model.predict(test_X,batch_size = 32,verbose= True)

"""##Ploting ROC curves"""

from collections import Counter
Counter(test_Y[0])

from sklearn.metrics import roc_curve, auc
fig, c_ax = plt.subplots(1,1, figsize = (9, 9))
for (idx, c_label) in enumerate(all_labels):
    fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])
    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))
c_ax.legend()
c_ax.set_xlabel('False Positive Rate')
c_ax.set_ylabel('True Positive Rate')
fig.savefig('barely_trained_net.png')

"""## Heat map using Grad Cam"""

def find_target_layer():
  for layer in reversed(multi_disease_model.layers):
    if len(layer.output_shape) == 4:
      return layer.name

layer=find_target_layer()
#layer='block3_conv3'
print(layer)

def gen_heatmap(img1,target_class):
  last_conv = multi_disease_model.get_layer(layer)
  grads = K.gradients(multi_disease_model.output[:,target_class],last_conv.output)[0]
  pooled_grads = K.mean(grads,axis=(0,1,2))
  iterate = K.function([multi_disease_model.input],[pooled_grads,last_conv.output[0]])
  pooled_grads_value,conv_layer_output = iterate([img1])
  for i in range(pooled_grads_value.shape[0]):
    conv_layer_output[:,:,i] *= pooled_grads_value[i]
  heatmap = np.mean(conv_layer_output,axis=-1)
  eps=1e-8
  numer = heatmap - np.min(heatmap)
  denom = (heatmap.max() - heatmap.min()) + eps
  heatmap = numer / denom
  upsample = resize(heatmap, IMG_SIZE,preserve_range=True)
  return upsample

img_index=all_xray_df['Image Index'].sample(16).tolist()
base_path='drive/My Drive/Chest_data/sample/images/'

img_array=np.zeros((16,128,128,3))
img_list=[]
for i in range(16) :
  img_path=base_path+img_index[i]
  img=cv2.imread(img_path)
  img=cv2.resize(img,IMG_SIZE)
  #plt.imshow(img)
  img_list.append(img)
  img_array[i] = img_to_array(img,dtype='float32')

predict = multi_disease_model.predict(img_array)
target_class=[]
for p in range(predict.shape[0]):
  target_class.append(np.argmax(predict[p]))
  print("Target Class = %d"%target_class[p])

fig, axs = plt.subplots(4, 4,figsize=(16,16))
for i in range(4):
  for j in range(4):
    k=4*i+j
    heatmap=gen_heatmap(img_array[k:k+1],target_class[k])
    axs[i,j].imshow(img_list[k])
    axs[i,j].imshow(heatmap,alpha=0.5)